\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics, multirow}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\parindent0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\let\mbb\boldsymbol
\renewcommand\boldsymbol{\mbb}
\renewcommand{\a}{\"{a}}
\renewcommand{\o}{\"{o}}
\renewcommand{\u}{\"{u}}
\newcommand{\beequal}{\mathop{=}\limits^!}
\newcommand{\equalstar}{\mathop{=}\limits^{(1)}}
\newcommand{\equalsstar}{\mathop{=}\limits^{(2)}}
\newcommand{\equalssstar}{\mathop{=}\limits^{(3)}}
\newcommand{\rayx}{\Lambda_A(\mbb{x})}
\newcommand{\rayv}{\Lambda_A(\mbb{v})}
\newcommand{\dray}{\nabla \Lambda_A(\mbb{x})}

\begin{document}

\title{Numerische Mathematik f\u r Ingenieure (SS 14) - \"{U}bung 7}
\author{Merikan Koyun \& Julian Andrej}
\maketitle

\section*{T12. Eigenwerte und Eigenvektoren des eindimensionalen Modellproblems}
Es seien $A \in \mathbb{R}^{n \times n}$, $1 \leq n \in \mathbb{N}, h := \frac{1}{n+1}$:
\begin{equation}
A=h^{-2}
\begin{pmatrix}
2 & -1 & & \\
-1 & \ddots & \ddots & \\
 & \ddots & \ddots & -1 \\
 & & -1 & 2 
\end{pmatrix}
\end{equation}
Definiere den Vektor $e^k \in \mathbb{R}^n$
\begin{equation}
e^k_j := \sin(\pi jkh) \text{ f\u r alle } j,k\in \{1,...,n\}
\end{equation}
und
\begin{equation}
\lambda_k := 4h^{-2} \sin^2(\pi kh/2) \text{ f\u r alle } k\in \{1,...,n\} .
\end{equation}
Zeige, dass
\begin{equation}
Ae^k = \lambda_ke^k
\end{equation}
gilt.\vspace{0.2cm}

Wir betrachten eine beliebige $j$-te Komponente von $(Ae^k)_j$. Mit
\begin{equation}
A_{ij} =
 \begin{cases}
   2h^{-2}  & \text{falls } i=j \\
   -h^{-2}  & \text{falls } |i-j|=1 \\
   0 & \text{sonst }
  \end{cases}
\end{equation}
ergibt sich:
\begin{align*}
(Ae^k)_j 
&= h^{-2} ( 2e^k_j - e^k_{j-1} - e^k_{j+1} ) \\
&= h^{-2} ( 2\sin(\pi jkh) - \sin(\pi (j-1)kh) - \sin(\pi (j+1)kh) \\
&= h^{-2} ( 2\sin(\pi jkh) - \sin(\pi jkh - \pi kh) - \sin(\pi jkh + \pi kh) \\
&\equalstar h^{-2} ( 2\sin(\pi jkh) - \sin(\pi jkh)\cos(- \pi kh) -\cos(\pi jkh)\sin(-\pi kh) \\ & \hspace{0.3cm}- \sin(\pi jkh)\cos(\pi kh) - \cos(\pi jkh)\sin(\pi kh)) \\
&\equalsstar h^{-2} ( 2\sin(\pi jkh) - 2\sin(\pi jkh)\cos(\pi kh)) \\
&= h^{-2}\sin(\pi jkh) ( 2 - 2\cos(\pi kh)) \\
&\equalssstar h^{-2}\sin(\pi jkh)  4\sin^2(\pi kh/2) \\ 
&= \underbrace{4h^{-2}\sin^2(\pi kh/2)}_{\lambda_k}\underbrace{\sin(\pi jkh)}_{e^k_j} = \lambda_k e^k_j
\end{align*}
mit folgenden Theoremen:
\begin{align*}
\text{\scriptsize{(1):}}& \qquad \sin(x+y) = \sin(x)\cos(y) + \cos(x)\sin(y) \\ 
\text{\scriptsize{(2):}}& \qquad \sin(-x) = -\sin(x) \text{ und } \cos(-x) = \cos(x)\\ 
\text{\scriptsize{(3):}}& \qquad 2-2\cos(x) = 4\sin^2(x/2) 
\end{align*}



\section*{T13. Hauptachsentransformation}
\begin{itemize}
\item[a)]
Es soll gezeigt werden, dass ein Vektor $e\in \mathbb{R}^n \backslash \{0\}$ und ein $\lambda\in \mathbb{R}$ existieren mit $Ae=\lambda e$.

Der Satz von Heine-Borel besagt, dass, wenn  $A \in \mathbb{R}^{n \times n}, A=A^*$, $1 \leq n \in \mathbb{N}$, ein Vektor $v\in  \mathbb{R}^n$, $\Vert v\Vert_2 = 1$ existiert, mit $\rayv \geq \rayx$ f\u r alle $x\in \mathbb{R}^n \backslash \{0\}$. Heine-Borel besagt, dass $v$ ein Punkt ist, in dem $\rayv$ ein Maximum besitzt. Der Satz in T11b) zeigt, dass dann $\lambda = \rayv$ ein Eigenwert zum Eigenvektor $v$ ist, sodass gilt $Av=\lambda v$. Da $\Vert v\Vert_2 = 1$ gilt, folgt $v \neq 0$. Somit gilt $e=v$ und die Behauptung wurde gezeigt. 

\item[b)]
Sei $A \in \mathbb{R}^{n \times n}, A=A^*$ und $\hat{e} = (1, 0, ..., 0)^* \in \mathbb{R}^n$. Es soll gezeigt werden, dass eine Householder-Matrix $H \in \mathbb{R}^{n \times n}$ mit $He=\alpha \hat{e}, \alpha \in \mathbb{R}$ und ein $\hat{A} \in \mathbb{R}^{(n-1) \times (n-1)}$ mit
\begin{equation}
HAH^* =
\left(
\begin{array}{c|ccc}
\lambda & 0 & \hdots & 0    \\\hline
0  & & &                    \\
\vdots & & \hat{A} &        \\
0  & & &
\end{array}
\right)
\end{equation}
existieren.\vspace{0.3cm}

$H_1e = \alpha \hat{e}$ spiegelt die erste Spalte von $A$ auf ein Vielfaches des ersten Einheitsvektors. Es gilt also (siehe T13a) dass $\Vert e \Vert_2 = 1$, da $\Vert v \Vert_2 = 1$ und somit $\alpha=1$. $H$ ist, wie in T6 bewiesen, orthogonal.
 
Mit $\hat{e}=(1,0,\hdots,0)^*$ und $\alpha=1$ und der Symmetrie von $A$ und $H$, sowie Orthogonalit\a t von $H$, ergibt sich:

\begin{align}
HAH^* 
&=
H
\left(
\begin{array}{c|ccc}
\lambda e &  & \lambda e &     \\\hline
  & & &                    \\
\lambda e & & \times &        \\
  & & &
\end{array}
\right)\\
&=
\left(
\begin{array}{c|ccc}
\alpha\lambda\hat{e} &  & \alpha\lambda\hat{e} &     \\\hline
  & & &                    \\
\alpha\lambda\hat{e} & & \times &        \\
  & & &
\end{array}
\right)\\
&=
\left(
\begin{array}{c|ccc}
\lambda &  0 & \hdots &   0  \\\hline
0  & & &                    \\
\vdots & & \hat{A} &        \\
0  & & &
\end{array}
\right)\\
\end{align}


\item[c)]
Es soll bewiesen werden, dass eine orthogonale Matrix $U \in \mathbb{R}^{n \times n}$ und eine Diagonalmatrix $D \in \mathbb{R}^{n \times n}$  existieren mit $UAU^*=D$. 

Der Beweis erfolgt mit vollst\a ndiger Induktion.

Induktionsanfang $n = 2$:\\
Man w\a hlt $U$ als Householdermatrix, also $U=H$. Es gilt:
\begin{equation}
HAH^* =
\begin{pmatrix}
\lambda & 0 \\
0       & \times
\end{pmatrix}
\end{equation}
und somit ist $D$ diagonal.\\

Induktionsannahme: $UAU^*=D$ gilt f\u r $n$.\\

Induktionsschritt: Wir nehmen an, dass $\tilde{A}\in \mathbb{R}^{(n+1)\times(n+1)}$ und $\tilde{A}=\tilde{A}^*$, sodass auch alle Hauptuntermatrizen von $\tilde{A}$ symmetrisch sind. Mit Hilfe von b) existiert auch eine Householdermatrix $\tilde{H} \in \mathbb{R}^{(n+1)\times(n+1)}$.
Es gilt also:
\begin{equation}
\tilde{H}\tilde{A}\tilde{H}^* = 
\left(
\begin{array}{c|ccc}
\lambda & 0 & \hdots & 0    \\\hline
0  & & &                    \\
\vdots & & \hat{A} &        \\
0  & & &
\end{array}
\right)
= \tilde{D}
\end{equation}
wobei $\hat{A}$ eine symmetrische $n \times n$-Matrix ist. Laut Induktionsannahme, ist $\hat{A}$ diagonalisierbar, sodass gilt:
\begin{align}
C\hat{A}C^*=D 
\end{align}


Somit existiert eine orthogonale $(n+1)\times(n+1)$ Matrix $U$, sodass gilt:
\begin{equation}
U = 
\left(
\begin{array}{c|ccc}
1 & 0 & \hdots & 0    \\\hline
0  & & &                    \\
\vdots & & C &        \\
0  & & &
\end{array}
\right)
\tilde{H}
\end{equation}

$U$ ist orthogonal, da $\tilde{H}$ orthogonal ist und $C$ per Definition auch orthogonal sein muss. Es l\a sst sich einfach zeigen $UU^*=I$ (wegen Orthogonalit\a t).

Nun muss noch gezeigt werden, dass $U\tilde{A}U^*$ eine Diagonalmatrix ist. Einfacher zu zeigen ist, dass $U^*\hat{A}U$ eine Diagonalmatrix ist:
\begin{align*}
U^*\tilde{A}U 
&= 
\left[
\tilde{H}
\begin{pmatrix}
1 & 0 \\
0 & C
\end{pmatrix}\right]^*
\tilde{A}
\tilde{H}
\begin{pmatrix}
1 & 0 \\
0 & C
\end{pmatrix}\\
&=
\begin{pmatrix}
1 & 0 \\
0 & C
\end{pmatrix}^*
\tilde{H}^*
\tilde{A}
\tilde{H}
\begin{pmatrix}
1 & 0 \\
0 & C
\end{pmatrix}\\
&=
\begin{pmatrix}
1 & 0 \\
0 & C^*
\end{pmatrix}
\begin{pmatrix}
\lambda & 0\\
0 & \hat{A}
\end{pmatrix}
\begin{pmatrix}
1 & 0 \\
0 & C
\end{pmatrix}\\
&=
\begin{pmatrix}
1 & 0 \\
0 & C^*
\end{pmatrix}
\begin{pmatrix}
\lambda & 0\\
0 & \hat{A}C
\end{pmatrix}\\
&=
\begin{pmatrix}
\lambda & 0\\
0 & C\hat{A}C^*
\end{pmatrix}\\
&=
\begin{pmatrix}
\lambda & 0\\
0 & D
\end{pmatrix} _\blacksquare
\end{align*} 

\end{itemize}

\end{document}











