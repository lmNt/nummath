\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{graphics, multirow}
\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm
\parindent0cm
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem*{surfacecor}{Corollary 1}
\newtheorem{conjecture}{Conjecture}
\newtheorem{question}{Question}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\let\mbb\boldsymbol
\renewcommand\boldsymbol{\mbb}
\renewcommand{\a}{\"{a}}
\renewcommand{\o}{\"{o}}
\renewcommand{\u}{\"{u}}
\newcommand{\beequal}{\mathop{=}\limits^!}
\newcommand{\equalstar}{\mathop{=}\limits^{(1)}}
\newcommand{\equalsstar}{\mathop{=}\limits^{(2)}}
\newcommand{\equalssstar}{\mathop{=}\limits^{(3)}}
\newcommand{\rayx}{\Lambda_A(\mbb{x})}
\newcommand{\rayv}{\Lambda_A(\mbb{v})}
\newcommand{\dray}{\nabla \Lambda_A(\mbb{x})}

\begin{document}

\title{Numerische Mathematik f\u r Ingenieure (SS 14) - \"{U}bung 7}
\author{Merikan Koyun \& Julian Andrej}
\maketitle

\section*{T12. Eigenwerte und Eigenvektoren des eindimensionalen Modellproblems}
Es seien $A \in \mathbb{R}^{n \times n}$, $1 \leq n \in \mathbb{N}, h := \frac{1}{n+1}$:
\begin{equation}
A=h^{-2}
\begin{pmatrix}
2 & -1 & & \\
-1 & \ddots & \ddots & \\
 & \ddots & \ddots & -1 \\
 & & -1 & 2 
\end{pmatrix}
\end{equation}
Definiere den Vektor $e^k \in \mathbb{R}^n$
\begin{equation}
e^k_j := \sin(\pi jkh) \text{ f\u r alle } j,k\in \{1,...,n\}
\end{equation}
und
\begin{equation}
\lambda_k := 4h^{-2} \sin^2(\pi kh/2) \text{ f\u r alle } k\in \{1,...,n\} .
\end{equation}
Zeige, dass
\begin{equation}
Ae^k = \lambda_ke^k
\end{equation}
gilt.\vspace{0.2cm}

Wir betrachten eine beliebige $j$-te Komponente von $(Ae^k)_j$. Mit
\begin{equation}
A_{ij} =
 \begin{cases}
   2h^{-2}  & \text{falls } i=j \\
   -h^{-2}  & \text{falls } |i-j|=1 \\
   0 & \text{sonst }
  \end{cases}
\end{equation}
ergibt sich:
\begin{align*}
(Ae^k)_j 
&= h^{-2} ( 2e^k_j - e^k_{j-1} - e^k_{j+1} ) \\
&= h^{-2} ( 2\sin(\pi jkh) - \sin(\pi (j-1)kh) - \sin(\pi (j+1)kh) \\
&= h^{-2} ( 2\sin(\pi jkh) - \sin(\pi jkh - \pi kh) - \sin(\pi jkh + \pi kh) \\
&\equalstar h^{-2} ( 2\sin(\pi jkh) - \sin(\pi jkh)\cos(- \pi kh) -\cos(\pi jkh)\sin(-\pi kh) \\ & \hspace{0.3cm}- \sin(\pi jkh)\cos(\pi kh) - \cos(\pi jkh)\sin(\pi kh)) \\
&\equalsstar h^{-2} ( 2\sin(\pi jkh) - 2\sin(\pi jkh)\cos(\pi kh)) \\
&= h^{-2}\sin(\pi jkh) ( 2 - 2\cos(\pi kh)) \\
&\equalssstar h^{-2}\sin(\pi jkh)  4\sin^2(\pi kh/2) \\ 
&= \underbrace{4h^{-2}\sin^2(\pi kh/2)}_{\lambda_k}\underbrace{\sin(\pi jkh)}_{e^k_j} = \lambda_k e^k_j
\end{align*}
mit folgenden Theoremen:
\begin{align*}
\text{\scriptsize{(1):}}& \qquad \sin(x+y) = \sin(x)\cos(y) + \cos(x)\sin(y) \\ 
\text{\scriptsize{(2):}}& \qquad \sin(-x) = -\sin(x) \text{ und } \cos(-x) = \cos(x)\\ 
\text{\scriptsize{(3):}}& \qquad 2-2\cos(x) = 4\sin^2(x/2) 
\end{align*}



\section*{T13. Hauptachsentransformation}
\begin{itemize}
\item[a)]
Es soll gezeigt werden, dass ein Vektor $e\in \mathbb{R}^n \backslash \{0\}$ und ein $\lambda\in \mathbb{R}$ existieren mit $Ae=\lambda e$.

Der Satz von Heine-Borel besagt, dass, wenn  $A \in \mathbb{R}^{n \times n}, A=A^*$, $1 \leq n \in \mathbb{N}$, ein Vektor $v\in  \mathbb{R}^n$, $\Vert v\Vert_2 = 1$ existiert, mit $\rayv \geq \rayx$ f\u r alle $x\in \mathbb{R}^n \backslash \{0\}$. Heine-Borel besagt, dass $v$ ein Punkt ist, in dem $\rayv$ ein Maximum besitzt. Der Satz in T11b) zeigt, dass dann $\lambda = \rayv$ ein Eigenwert zum Eigenvektor $v$ ist, sodass gilt $Av=\lambda v$. Da $\Vert v\Vert_2 = 1$ gilt, folgt $v \neq 0$. Somit gilt $e=v$ und die Behauptung wurde gezeigt. 

KP OP DAS REICHT.
\item[b)]
Sei $A \in \mathbb{R}^{n \times n}, A=A^*$ und $\hat{e} = (1, 0, ..., 0)^* \in \mathbb{R}^n$. Es soll gezeigt werden, dass eine Householder-Matrix $H \in \mathbb{R}^{n \times n}$ mit $He=\alpha \hat{e}, \alpha \in \mathbb{R}$ und ein $\hat{A} \in \mathbb{R}^{(n-1) \times (n-1)}$ mit
\begin{equation}
HAH^* =
\left(
\begin{array}{c|ccc}
\lambda & 0 & \hdots & 0    \\\hline
0  & & &                    \\
\vdots & & \hat{A} &        \\
0  & & &
\end{array}
\right)
\end{equation}
existieren.\vspace{0.3cm}

MAH PROOF!

$H_1e = \alpha \hat{e}$ spiegelt die erste Spalte von $A$ auf ein Vielfaches des ersten Einheitsvektors. Laut Vorlesung (4.6)/(4.7) bzw. T6b) ist der erste Einheitsvektor ein Eigenvektor zum Eigenwert $\lambda_1$. $H$ ist per Definition orthogonal.
Man definiert einen normierten Eigenvektor $v_1$ von $A$ zum Eigenwert $\lambda$. Bei der Anwendung der Householder-Matrix $H$ auf die symmetrische Matrix $A$ ergibt sich SHIAT.

PROOF KOPIERT AUS SKRIPT UNI SAARBRUECKEN! UMSCHREIBEN

Zu einem reellen Eigenwert $\lambda_1$ w\a hle man einen Eigenvektor $v$ der L\a nge eins und erg\a nze ihn zu einer Orthonormalbasis $(v, w_2, ..., w_n)$. Ist $H$ die Matrix mit diesen Spaltenvektoren, so gilt:
\begin{align}
HAH^* &= 
\begin{pmatrix}
v^T \\
w_2^T\\
\vdots\\
w_n^T
\end{pmatrix}
A
\begin{pmatrix}
v & w_2 & \hdots & w_n
\end{pmatrix} \\
&=
\begin{pmatrix}
v^T \\
w_2^T\\
\vdots\\
w_n^T
\end{pmatrix}
\begin{pmatrix}
\lambda v & Aw_2 & \hdots & Aw_n
\end{pmatrix}\\
&=
\begin{pmatrix}
\lambda v^Tv & v^TAw_2 & \hdots & v^TAw_n \\ 
\lambda w_2^Tv & w_2^TAw_2 & \hdots & w_2^TAw_n \\ 
\vdots & \vdots & & \vdots \\
\lambda w_n^Tv & w_n^TAw_2 & \hdots & w_n^TAw_n \\ 
\end{pmatrix}
\end{align}

Da $v$ auf die L\a nge eins normiert ist und senkrecht auf $w_i, i=2,\hdots,n$ steht, ist der oberste Eintrag in der ersten Spalte $\lambda$, alle anderen verschwinden. In der ersten Zeile beachtet man, dass aufgrund der Symmetrie von $A$ f\u r $i=2,\hdots,n$ gilt:
\begin{equation}
v^T A w_i = (A^Tv)^Tw_i = (Av)^Tw_i = \lambda v^Tw_i = 0
\end{equation}
wobei in der letzten Gleichheit wieder die Orthogonalit\a t ausgenutzt wurde. Es ist gezeigt
\begin{equation}
HAH^* = 
\begin{pmatrix}
\lambda & 0 & \hdots & 0 \\ 
0 & w_2^TAw_2 & \hdots & w_2^TAw_n \\ 
\vdots & \vdots & & \vdots \\
0 & w_n^TAw_2 & \hdots & w_n^TAw_n \\ 
\end{pmatrix}
\end{equation}
wobei die verbleibende $(n-1)\times(n-1)$ Restmatrix $\hat{A}$ aufgrund der Symmetrie selbst wieder symmetrisch ist.

\item[c)]
Es soll bewiesen werden, dass eine orthogonale Matrix $U \in \mathbb{R}^{n \times n}$ und eine Diagonalmatrix $D \in \mathbb{R}^{n \times n}$  existieren mit $UAU^*=D$. 

Der Beweis erfolgt mit vollst\a ndiger Induktion.

$n=1$: Trivial

$n\leq 2$.

\end{itemize}





\end{document}











